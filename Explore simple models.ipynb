{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from utility_funcitons import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts with number....\n",
      "Clickbait Phrases....\n",
      "Clickbait re....\n",
      "Num dots....\n",
      "Text Features....\n",
      "Punctuation....\n",
      "Word ratios....\n",
      "Sentiment Scores....\n",
      "Readability Scores....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276be5c0c90749aa9872b000085b333b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a3674826b84be8bf0359aec5059ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Glove.....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad2f15fa4074b6f8bd8bcf5ded363aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb02fd962b274ee38b45bab82d73ee90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "from feature_selection import *\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from utility_funcitons import *\n",
    "\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "train_features, test_features, feature_names = featurize(train, test, 'tfidf_glove')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(train.label.values == 'clickbait', 1, 0)\n",
    "y_test = np.where(test.label.values == 'clickbait', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "\n",
    "def adjusted_f1(y_true, y_prob):\n",
    "    f1 = print_model_metrics(y_true, y_prob, verbose = 0, return_metrics = True)[0]\n",
    "    return f1\n",
    "\n",
    "score = make_scorer(adjusted_f1, greater_is_better = True, needs_proba = True)\n",
    "\n",
    "\n",
    "\n",
    "# Since we want to use a predefined Test/Val set, we'll use PredefinedSplit and pass it as the CV parameter\n",
    "# We need to merge both the datasets and label 0 for test and -1 for the train set\n",
    "\n",
    "X = sparse.vstack((train_features, test_features))\n",
    "test_fold = [-1 for _ in range(train_features.shape[0])] + [0 for _ in range(test_features.shape[0])]\n",
    "y = np.concatenate([y_train, y_test])\n",
    "ps = PredefinedSplit(test_fold)\n",
    "\n",
    "def run_grid_search(model, params, x_train, y_train):\n",
    "    grid = GridSearchCV(model, params, cv = ps, n_jobs = -1, scoring = score, verbose = 0, refit = False)\n",
    "    grid.fit(x_train, y_train)\n",
    "    return (grid.best_params_, grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_n_times(model, x_train, y_train, x_test, y_test, n_iters = 10):\n",
    "    metrics = np.zeros(5)\n",
    "    for _ in range(n_iters):\n",
    "        model.fit(x_train, y_train)\n",
    "        y_test_prob = model.predict_proba(x_test)[:,1]\n",
    "        metrics += print_model_metrics(y_test, y_test_prob, verbose = False, return_metrics = True)\n",
    "    metrics /=10\n",
    "    print('F1: {:.3f} | Pr: {:.3f} | Re: {:.3f} | AUC: {:.3f} | Accuracy: {:.3f} \\n'.format(*metrics))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters : {'alpha': 0.0001, 'l1_ratio': 0.5, 'penalty': 'l2'}\n",
      "F1: 0.987 | Pr: 0.985 | Re: 0.988 | AUC: 0.999 | Accuracy: 0.987 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "lr = SGDClassifier(loss = 'log')\n",
    "lr_params = {'alpha' : [10**(-x) for x in range(7)],\n",
    "             'penalty' : ['l1', 'l2', 'elasticnet'],\n",
    "             'l1_ratio' : [0.15, 0.25, 0.5, 0.75]}\n",
    "\n",
    "best_params, best_f1 = run_grid_search(lr, lr_params, X, y)\n",
    "\n",
    "print('Best Parameters : {}'.format(best_params))\n",
    "\n",
    "lr = SGDClassifier(loss = 'log', \n",
    "                   alpha = best_params['alpha'], \n",
    "                   penalty = best_params['penalty'], \n",
    "                   l1_ratio = best_params['l1_ratio'])\n",
    "fit_n_times(lr, train_features, y_train, test_features, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters : {'C': 10, 'degree': 2, 'kernel': 'rbf'}\n",
      "Best F1 : 0.9898596333483191\n",
      "F1: 0.990 | Pr: 0.988 | Re: 0.991 | AUC: 0.999 | Accuracy: 0.990 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(probability = True)\n",
    "svm_params = {'C' : [10**(x) for x in range(-1,4)],\n",
    "             'kernel' : ['poly', 'rbf', 'linear'],\n",
    "             'degree' : [2, 3]}\n",
    "\n",
    "best_params, best_f1 = run_grid_search(svm, svm_params, X, y)\n",
    "\n",
    "print('Best Parameters : {}'.format(best_params))\n",
    "print('Best F1 : {}'.format(best_f1))\n",
    "\n",
    "svm = SVC(C = best_params['C'], kernel = best_params['kernel'], degree = best_params['degree'], probability = True)\n",
    "fit_n_times(svm, train_features, y_train, test_features, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters : {'alpha': 100000}\n",
      "Best F1 : 0.9467467368107598\n",
      "F1: 0.947 | Pr: 0.939 | Re: 0.954 | AUC: 0.988 | Accuracy: 0.946 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB(class_prior = [0.5, 0.5])\n",
    "nb_params = {'alpha' : [10**(x) for x in range(6)]}\n",
    "\n",
    "\n",
    "best_params, best_f1 = run_grid_search(nb, nb_params, X, y)\n",
    "\n",
    "print('Best Parameters : {}'.format(best_params))\n",
    "print('Best F1 : {}'.format(best_f1))\n",
    "\n",
    "nb = MultinomialNB(alpha = best_params['alpha'], class_prior = [0.5, 0.5])\n",
    "\n",
    "fit_n_times(nb, train_features, y_train, test_features, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_jobs = -1)\n",
    "\n",
    "knn_params = { 'n_neighbors' : [3, 5, 7, 9, 15, 31], \n",
    "               'weights' : ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "best_params, best_f1 = run_grid_search(knn, knn_params, X, y)\n",
    "print('Best Parameters : {}'.format(best_params))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = best_params['n_neighbors'], weights = best_params['weights'], n_jobs = -1)\n",
    "\n",
    "fit_n_times(knn, train_features, y_train, test_features, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starts with number....\n",
    "Clickbait Phrases....\n",
    "Clickbait re....\n",
    "Num dots....\n",
    "Text Features....\n",
    "Punctuation....\n",
    "Word ratios....\n",
    "Sentiment Scores....\n",
    "Readability Scores....\n",
    "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))\n",
    "\n",
    "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))\n",
    "\n",
    "Glove.....\n",
    "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))\n",
    "\n",
    "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))\n",
    "\n",
    "DONE!\n",
    "Fitting 1 folds for each of 12 candidates, totalling 12 fits\n",
    "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
    "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  4.2min finished\n",
    "Best Parameters : {'n_neighbors': 7, 'weights': 'distance'}\n",
    "F1: 0.984 | Pr: 0.980 | Re: 0.988 | AUC: 0.994 | Accuracy: 0.984 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters : {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 500}\n",
      "F1: 0.982 | Pr: 0.979 | Re: 0.984 | AUC: 0.998 | Accuracy: 0.982 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs = -1)\n",
    "\n",
    "rf_params = { 'n_estimators' : [10, 100, 250, 500, 1000], \n",
    "               'max_depth' : [None, 3, 7, 15],\n",
    "               'min_samples_split' : [2, 5, 15]\n",
    "}\n",
    "\n",
    "best_params, best_f1 = run_grid_search(rf, rf_params, X, y)\n",
    "\n",
    "print('Best Parameters : {}'.format(best_params))\n",
    "rf = RandomForestClassifier(n_estimators = best_params['n_estimators'],\n",
    "                            min_samples_split = best_params['min_samples_split'],\n",
    "                            max_depth = best_params['max_depth'], \n",
    "                            n_jobs = -1)\n",
    "fit_n_times(rf, train_features, y_train, test_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters : {'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 500, 'reg_alpha': 0}\n",
      "F1: 0.986 | Pr: 0.982 | Re: 0.991 | AUC: 0.999 | Accuracy: 0.986 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(n_jobs = -1)\n",
    "\n",
    "xgb_params = { 'n_estimators' : [10, 100, 200, 500], \n",
    "               'max_depth' : [1, 2, 3, 7],\n",
    "               'learning_rate' : [0.1, 0.2, 0.01, 0.3],\n",
    "               'reg_alpha' : [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "best_params, best_f1 = run_grid_search(xgb, xgb_params, X, y)\n",
    "\n",
    "print('Best Parameters : {}'.format(best_params))\n",
    "\n",
    "xgb = XGBClassifier(n_estimators = best_params['n_estimators'],\n",
    "                            learning_rate = best_params['learning_rate'],\n",
    "                            max_depth = best_params['max_depth'], \n",
    "                            reg_alpha = best_params['reg_alpha'], n_jobs = -1)\n",
    "\n",
    "fit_n_times(xgb, train_features.todense(), y_train, test_features.todense(), y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
